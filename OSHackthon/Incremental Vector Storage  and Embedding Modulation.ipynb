{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04465a0",
   "metadata": {},
   "source": [
    "# Incremental Vector Storage  and Embedding Modulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e740ce",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6bf316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (0.0.292)\n",
      "Requirement already satisfied: faiss-cpu in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (1.7.4)\n",
      "Requirement already satisfied: pypdf in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (3.16.0)\n",
      "Collecting GitPython\n",
      "  Obtaining dependency information for GitPython from https://files.pythonhosted.org/packages/f9/94/1877b88fa3a0a30bedb43757a14f548c3b2719c8d83c16012f89564c0f3b/GitPython-3.1.36-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.36-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (4.33.2)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.6.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (0.0.37)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (2.8.6)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: tqdm in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (1.11.2)\n",
      "Requirement already satisfied: nltk in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sentence-transformers) (0.17.1)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from llama-cpp-python) (4.7.1)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: fsspec in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: click in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/apple/anaconda3/envs/Medical-NLP/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.6-cp311-cp311-macosx_13_0_arm64.whl size=851328 sha256=f55ebaaafa4f1c349a505d93297e5ce75cb8efc038ed9dcec88d2e83c4cc6a75\n",
      "  Stored in directory: /Users/apple/Library/Caches/pip/wheels/18/f3/e6/e6d374c76db44b5b0451c3a76b3049f29e881819bc43f53d4d\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: smmap, et-xmlfile, diskcache, openpyxl, llama-cpp-python, gitdb, GitPython\n",
      "Successfully installed GitPython-3.1.36 diskcache-5.6.3 et-xmlfile-1.1.0 gitdb-4.0.10 llama-cpp-python-0.2.6 openpyxl-3.1.2 smmap-5.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain faiss-cpu pypdf GitPython openpyxl sentence-transformers transformers llama-cpp-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63cc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import (\n",
    "    LlamaCppEmbeddings, \n",
    "    HuggingFaceEmbeddings, \n",
    "    SentenceTransformerEmbeddings\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    DataFrameLoader,\n",
    "    GitLoader\n",
    "  )\n",
    "import pandas as pd\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5dfa4",
   "metadata": {},
   "source": [
    "## Text Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9b5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_splits(text_file):\n",
    "  \"\"\"Function takes in the text data and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  with open(text_file,'r') as txt:\n",
    "    data = txt.read()\n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = textSplit.split_text(data)\n",
    "  return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672e401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa07a045",
   "metadata": {},
   "source": [
    "## Pdf Text Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f257aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/Users/apple/Documents/TestingEmbbeding/9780723434177.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16db3c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b6fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pdf_splits(pdf_file):\n",
    "  \"\"\"Function takes in the pdf data and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  \n",
    "  loader = PyPDFLoader(pdf_file)\n",
    "  pages = loader.load_and_split()  \n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=850,\n",
    "                                             chunk_overlap=200,\n",
    "                                             length_function=len)\n",
    "  doc_list = []\n",
    "  #Pages will be list of pages, so need to modify the loop\n",
    "  for pg in pages:\n",
    "    pg_splits = textSplit.split_text(pg.page_content)# here we are giveing each page content with is a text\n",
    "    doc_list.extend(pg_splits)\n",
    "\n",
    "  return doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d6219",
   "metadata": {},
   "source": [
    "## Excel Text Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb6735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excel_splits(excel_file,target_col,sheet_name):\n",
    "  trialDF = pd.read_excel(io=excel_file,\n",
    "                          engine='openpyxl',\n",
    "                          sheet_name=sheet_name)\n",
    "  \n",
    "  df_loader = DataFrameLoader(trialDF,\n",
    "                              page_content_column=target_col)\n",
    "  \n",
    "  excel_docs = df_loader.load()\n",
    "\n",
    "  return excel_docs\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218502a",
   "metadata": {},
   "source": [
    "## Coma Seperated Values Text Extraction Fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd8a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_splits(csv_file):\n",
    "  \"\"\"Function takes in the csv and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  csvLoader = CSVLoader(csv_file)\n",
    "  csvdocs = csvLoader.load()\n",
    "  return csvdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfd69e",
   "metadata": {},
   "source": [
    "## IPYNB Text Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e90d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ipynb_splits(notebook):\n",
    "  \"\"\"Function takes the notebook file,reads the file \n",
    "  data as python script, then splits script data directly\"\"\"\n",
    "\n",
    "  with open(notebook) as fh:\n",
    "    nb = nbformat.reads(fh.read(), nbformat.NO_CONVERT)\n",
    "\n",
    "  exporter = PythonExporter()\n",
    "  source, meta = exporter.from_notebook_node(nb)\n",
    "\n",
    "  #Python file data is in the source variable\n",
    "  \n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = textSplit.split_text(source)\n",
    "  return doc_lis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752d639",
   "metadata": {},
   "source": [
    "## Git Hub File Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c12b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_files(repo_link, folder_path, file_ext):\n",
    "  # eg. loading only python files\n",
    "  git_loader = GitLoader(clone_url=repo_link,\n",
    "    repo_path=folder_path, \n",
    "    file_filter=lambda file_path: file_path.endswith(file_ext))\n",
    "  #Will take each file individual document\n",
    "  git_docs = git_loader.load()\n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = []\n",
    "  #Pages will be list of pages, so need to modify the loop\n",
    "  for code in git_docs:\n",
    "    code_splits = textSplit.split_text(code.page_content)\n",
    "    doc_list.extend(code_splits)\n",
    "\n",
    "  return doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946910d",
   "metadata": {},
   "source": [
    "## Incrementation of Exsisting Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7532b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_index(doc_list, embed_fn, index_store):\n",
    "  \"\"\"Function takes in existing vector_store, \n",
    "  new doc_list and embedding function that is \n",
    "  initialized on appropriate model. Local or online. \n",
    "  New embedding is merged with the existing index. If no \n",
    "  index given a new one is created\"\"\"\n",
    "  #check whether the doc_list is documents, or text\n",
    "  try:\n",
    "    faiss_db = FAISS.from_documents(doc_list, \n",
    "                              embed_fn)  \n",
    "  except Exception as e:\n",
    "    faiss_db = FAISS.from_texts(doc_list, \n",
    "                              embed_fn)\n",
    "  \n",
    "  if os.path.exists(index_store):\n",
    "    local_db = FAISS.load_local(index_store,embed_fn)\n",
    "    #merging the new embedding with the existing index store\n",
    "    local_db.merge_from(faiss_db)\n",
    "    print(\"Merge completed\")\n",
    "    local_db.save_local(index_store)\n",
    "    print(\"Updated index saved\")\n",
    "  else:\n",
    "    faiss_db.save_local(folder_path=index_store)\n",
    "    print(\"New store created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a367b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_docs_length(index_path, embed_fn):\n",
    "  test_index = FAISS.load_local(index_path,\n",
    "                              embeddings=embed_fn)\n",
    "  test_dict = test_index.docstore._dict\n",
    "  return len(test_dict.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a8ef88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing out the above function with the open source \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "639fbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDC_documents = get_pdf_splits(\"/Users/apple/Desktop/BookAI 2/BDC.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a69dcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BharatAI_documents = get_pdf_splits(\"/Users/apple/Desktop/Protoype/finalppt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "124bc742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BDC_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83321f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BharatAI_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a5adcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New store created...\n"
     ]
    }
   ],
   "source": [
    "embed_index(doc_list=BDC_documents,embed_fn=embeddings,index_store='new_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b84f5916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed\n",
      "Updated index saved\n"
     ]
    }
   ],
   "source": [
    "embed_index(doc_list=BharatAI_documents,embed_fn=embeddings,index_store='new_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8886c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_docs_length(index_path='new_index',embed_fn=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc745273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_index = FAISS.load_local(\"new_index\",embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddd3309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='SRM’s Bharat AI Navigating Mining Laws with Ease', metadata={})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result = test_index.similarity_search(\"What is bharat ai\")\n",
    " result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bf653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
